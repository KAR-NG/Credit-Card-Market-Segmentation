---
title: "Credit card Market Segmentation by Auto-Clustering"
author: "Kar Ng"
date: '2022-05'
output: 
  github_document: 
    toc: true
    toc_depth: 4
always_allow_html: yes
---


## 1 R PACKAGES

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(kableExtra)
library(skimr)
library(factoextra)
library(hopkins)
library(clValid)

```


## 2 INTRODUCTION

This project will use clustering technique to develop a customer segmentation to define marketing strategy. 

Dataset used in this project is called "Credit Card Dataset for Clustering" by Arjun Bhasin. It is a public dataset acquired from [Kaggle.com](https://www.kaggle.com/datasets/arjunbhasin2013/ccdata).

The dataset has 17 behavioral information of about 9000 active credit card holders.

## 3 DATA PREPARATION

### 3.1 Data Import

Following codes import the dataset and specify the first column as row's name, as it is required to perform clustering. 

```{r}
cc <- read.csv("cc_dataset.csv",
               header = T,
               row.names = 1)   # specificy column 1 as row name 

```

randomly sample the first 10 rows of the dataset. 

```{r}
sample_n(cc, 10)

```

The name of all variables are:

```{r}
names(cc)

```


### 3.2 Data Description

Following is the data description extracted from the kaggle website.

```{r}

Variables <- c("CUSTID", "BALANCE", "BALANCEFREQUENCY", "PURCHASES", "ONEOFFPURCHASES",
               "INSTALLMENTSPURCHASES", "CASHADVANCE", "PURCHASESFREQUENCY", "ONEOFFPURCHASESFREQUENCY", "PURCHASESINSTALLMENTSFREQUENCY", "CASHADVANCEFREQUENCY", "CASHADVANCETRX", "PURCHASESTRX", "CREDITLIMIT", "PAYMENTS", "MINIMUM_PAYMENTS", "PRCFULLPAYMENT", "TENURE")

Description <- c("Identification of Credit Card holder (Categorical)",
                 "Balance amount left in their account to make purchases",
                 "How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)",
                 "Amount of purchases made from account",
                 "Maximum purchase amount done in one-go",
                 "Amount of purchase done in installment",
                 "Cash in advance given by the user",
                 "How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)",
                 "How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)",
                 "How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)",
                 "How frequently the cash in advance being paid",
                 "Number of Transactions made with Cash in Advanced",
                 "Number of purchase transactions made",
                 "Limit of Credit Card for user",
                 "Amount of Payment done by user",
                 "Minimum amount of payments made by user",
                 "Percent of full payment paid by user",
                 "Tenure of credit card service for user")


data.frame(Variables, Description) %>% 
  kbl() %>% 
  kable_styling(bootstrap_options = c("bordered", "stripped"))

```

### 3.3 Data Exploration

**Data Size and type**

The dataset contain 8950 rows and 17 variables. The "dbl" and "ind" are data type allocated by R to a particular column. The "dbl" stands for "double", it is used for numerical variables that have decimal places. The "int" stands for integer, it is used for numerical variables that have integer values. All variables are numeric with either "dbl" and "int", they will be treated the same type during analysis.

```{r}
glimpse(cc)

```
**Purchases**

During exploration, I found that "PURCHASES" is the sum of "ONEOFF_PURCHASES" and "INSTALLMENTS_PURCHASES". It may not be important in this analysis.

Following select 10 rows among the dataset, and the new variable "MY_PURCHASES" proves my finding, which is the same as the "PURCHASES", and is the sum of "ONEOFF_PURCHASES" and "INSTALLMENTS_PURCHASES".

```{r}
cc %>% 
  top_n(10, BALANCE) %>% 
  dplyr::select(ONEOFF_PURCHASES, INSTALLMENTS_PURCHASES, PURCHASES) %>% 
  mutate(MY_PURCHASES = ONEOFF_PURCHASES + INSTALLMENTS_PURCHASES)

```

**Missing values check**

Again, proven by other function that the tables have 8950 rows of data and 17 variables. All variables are numerical, and, by examining the variables "n_missing" and "complete_rate" in following tables, there is 1 missing value in the variable "CREDIT_LIMIT" and 313 in the "MINIMUM_PAYMENTS". These missing values need to be handled. 

```{r}
skim_without_charts(cc)

```
Alternatively, following code performs the missing-value check.

```{r}
colSums(is.na(cc))

```
There are 8950 rows of data and I will still have 96.5% of data left after removal of these missing values and therefore I will simply remove these missing values for the simplicity of this project.

Typically, missing value can be handled by either removal, replaced with mean, median, or using imputation algorithm such as KNN or bagging algorithm. These techniques are usually performed when there are too many missing values in important variables. For example, when missing values is higher than 5% and less than 60%. 


## 4 DATA CLEANING

### 4.1 Rename all variables

The name of all variables are in capital form and which would be difficult to read for readers and also myself.

```{r}
colnames(cc)
```
Following code transforms all the name into reading-friendly form. 

```{r}
cc <- cc %>% 
  rename_all(str_to_sentence)

```

Checking again the name of each variable.

```{r}
colnames(cc)

```

### 4.2 NA Removal

Following code remove all the missing values in the dataset (314 rows among 8950 rows)

```{r}
cc <- cc %>% 
  na.omit()

```

Now, the number of rows have been reduced to 8636 from 8950. 

```{r}
str(cc)
```

## 5 EDA

**Distribution Study**

A primary exploratory data analysis is suggested to quickly understand the general distribution of the data.  

```{r, fig.height=10, fig.width=13, message=FALSE, warning=FALSE}

# data frame

cc2 <- cc %>% 
  pivot_longer(c(1:17), 
               names_to = "my.variable", 
               values_to = "my.value")

# graphs

ggplot(cc2, aes(x = my.value, fill = my.variable)) +
  geom_histogram(color = "black") +
  facet_wrap(~my.variable, scale = "free") +
  theme_minimal() +
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 45),
        plot.title = element_text(face = "bold", hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5, face = "bold")) +
  labs(title = "Variable Distribution Analysis",
       subtitle = "by Histogram")


```

Most of the variables follow Pareto trend (80:20 rule) with majority of the data belong to one side of the value. it can be seem quite hard to group the data and categories the data into several distinct group for marketing purposes.


## 6 Clustering

The dataset must met several conditions prior to be clustered,

1. Observation as row and variable as variable, and it has been met.  
2. No missing values in the dataset, and it has been met.  
3. Standardising the data (generally recommended) to make variables comparable. This step will transform the data in all variables to a scale that would have 0 mean and 1 standard deviation. This step will be performed in this section.    

Standardising the dataset:

```{r}
cc.scale <- scale(cc)

nrow(cc.scale)
```

### 6.1 Clustering Tendency Assessment

There is a big issue with clustering techniques is that it will always return clusters even if there is no any cluster in the dataset. Therefore, clustering tendency assessment is an important step to test is there clusters in the dataset. 

**Hopkins Statistics** is a statistical method that using probability to test for spatial randomness of the data. The null hypothesis is that the credit card dataset is uniformly distributed, indicating no meaningful cluster. 

* If the value of hopkins statistic is low (0 - 3), it indicates regularly-spaced data  
* if the value is around zero, it indicates random data   
* If the value is around 0.7 to 1, it indicate clustered data  

```{r}
hopkins(cc.scale, m = nrow(cc.scale)-1)

```
The Hopkins Statistics is 1, and base on this value I can reject the null hypothesis and conclude that the credit card dataset is significantly a clusterable data.

### 6.2 Find Optimal K

After 


### 6.3 Hierarchical K-Means Clustering


```{r}
res.hk <- hkmeans(cc.scale, 4)

```


```{r}
res.hk

```



## REFERENCE



